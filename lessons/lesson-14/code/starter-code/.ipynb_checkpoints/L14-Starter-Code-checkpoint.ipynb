{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>urlid</th>\n",
       "      <th>boilerplate</th>\n",
       "      <th>alchemy_category</th>\n",
       "      <th>alchemy_category_score</th>\n",
       "      <th>avglinksize</th>\n",
       "      <th>commonlinkratio_1</th>\n",
       "      <th>commonlinkratio_2</th>\n",
       "      <th>commonlinkratio_3</th>\n",
       "      <th>commonlinkratio_4</th>\n",
       "      <th>...</th>\n",
       "      <th>linkwordscore</th>\n",
       "      <th>news_front_page</th>\n",
       "      <th>non_markup_alphanum_characters</th>\n",
       "      <th>numberOfLinks</th>\n",
       "      <th>numwords_in_url</th>\n",
       "      <th>parametrizedLinkRatio</th>\n",
       "      <th>spelling_errors_ratio</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bloomberg.com/news/2010-12-23/ibm-p...</td>\n",
       "      <td>4042</td>\n",
       "      <td>{\"title\":\"IBM Sees Holographic Calls Air Breat...</td>\n",
       "      <td>business</td>\n",
       "      <td>0.789131</td>\n",
       "      <td>2.055556</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>5424</td>\n",
       "      <td>170</td>\n",
       "      <td>8</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.079130</td>\n",
       "      <td>0</td>\n",
       "      <td>IBM Sees Holographic Calls Air Breathing Batte...</td>\n",
       "      <td>A sign stands outside the International Busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.popsci.com/technology/article/2012-...</td>\n",
       "      <td>8471</td>\n",
       "      <td>{\"title\":\"The Fully Electronic Futuristic Star...</td>\n",
       "      <td>recreation</td>\n",
       "      <td>0.574147</td>\n",
       "      <td>3.677966</td>\n",
       "      <td>0.508021</td>\n",
       "      <td>0.288770</td>\n",
       "      <td>0.213904</td>\n",
       "      <td>0.144385</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>4973</td>\n",
       "      <td>187</td>\n",
       "      <td>9</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.125448</td>\n",
       "      <td>1</td>\n",
       "      <td>The Fully Electronic Futuristic Starting Gun T...</td>\n",
       "      <td>And that can be carried on a plane without the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.menshealth.com/health/flu-fighting-...</td>\n",
       "      <td>1164</td>\n",
       "      <td>{\"title\":\"Fruits that Fight the Flu fruits tha...</td>\n",
       "      <td>health</td>\n",
       "      <td>0.996526</td>\n",
       "      <td>2.382883</td>\n",
       "      <td>0.562016</td>\n",
       "      <td>0.321705</td>\n",
       "      <td>0.120155</td>\n",
       "      <td>0.042636</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>2240</td>\n",
       "      <td>258</td>\n",
       "      <td>11</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>1</td>\n",
       "      <td>Fruits that Fight the Flu fruits that fight th...</td>\n",
       "      <td>Apples The most popular source of antioxidants...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.dumblittleman.com/2007/12/10-foolpr...</td>\n",
       "      <td>6684</td>\n",
       "      <td>{\"title\":\"10 Foolproof Tips for Better Sleep \"...</td>\n",
       "      <td>health</td>\n",
       "      <td>0.801248</td>\n",
       "      <td>1.543103</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2737</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.100858</td>\n",
       "      <td>1</td>\n",
       "      <td>10 Foolproof Tips for Better Sleep</td>\n",
       "      <td>There was a period in my life when I had a lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://bleacherreport.com/articles/1205138-the...</td>\n",
       "      <td>9006</td>\n",
       "      <td>{\"title\":\"The 50 Coolest Jerseys You Didn t Kn...</td>\n",
       "      <td>sports</td>\n",
       "      <td>0.719157</td>\n",
       "      <td>2.676471</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.043210</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>12032</td>\n",
       "      <td>162</td>\n",
       "      <td>10</td>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.082569</td>\n",
       "      <td>0</td>\n",
       "      <td>The 50 Coolest Jerseys You Didn t Know Existed...</td>\n",
       "      <td>Jersey sales is a curious business Whether you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  urlid  \\\n",
       "0  http://www.bloomberg.com/news/2010-12-23/ibm-p...   4042   \n",
       "1  http://www.popsci.com/technology/article/2012-...   8471   \n",
       "2  http://www.menshealth.com/health/flu-fighting-...   1164   \n",
       "3  http://www.dumblittleman.com/2007/12/10-foolpr...   6684   \n",
       "4  http://bleacherreport.com/articles/1205138-the...   9006   \n",
       "\n",
       "                                         boilerplate alchemy_category  \\\n",
       "0  {\"title\":\"IBM Sees Holographic Calls Air Breat...         business   \n",
       "1  {\"title\":\"The Fully Electronic Futuristic Star...       recreation   \n",
       "2  {\"title\":\"Fruits that Fight the Flu fruits tha...           health   \n",
       "3  {\"title\":\"10 Foolproof Tips for Better Sleep \"...           health   \n",
       "4  {\"title\":\"The 50 Coolest Jerseys You Didn t Kn...           sports   \n",
       "\n",
       "  alchemy_category_score  avglinksize  commonlinkratio_1  commonlinkratio_2  \\\n",
       "0               0.789131     2.055556           0.676471           0.205882   \n",
       "1               0.574147     3.677966           0.508021           0.288770   \n",
       "2               0.996526     2.382883           0.562016           0.321705   \n",
       "3               0.801248     1.543103           0.400000           0.100000   \n",
       "4               0.719157     2.676471           0.500000           0.222222   \n",
       "\n",
       "   commonlinkratio_3  commonlinkratio_4  \\\n",
       "0           0.047059           0.023529   \n",
       "1           0.213904           0.144385   \n",
       "2           0.120155           0.042636   \n",
       "3           0.016667           0.000000   \n",
       "4           0.123457           0.043210   \n",
       "\n",
       "                         ...                          linkwordscore  \\\n",
       "0                        ...                                     24   \n",
       "1                        ...                                     40   \n",
       "2                        ...                                     55   \n",
       "3                        ...                                     24   \n",
       "4                        ...                                     14   \n",
       "\n",
       "   news_front_page  non_markup_alphanum_characters  numberOfLinks  \\\n",
       "0                0                            5424            170   \n",
       "1                0                            4973            187   \n",
       "2                0                            2240            258   \n",
       "3                0                            2737            120   \n",
       "4                0                           12032            162   \n",
       "\n",
       "   numwords_in_url  parametrizedLinkRatio  spelling_errors_ratio label  \\\n",
       "0                8               0.152941               0.079130     0   \n",
       "1                9               0.181818               0.125448     1   \n",
       "2               11               0.166667               0.057613     1   \n",
       "3                5               0.041667               0.100858     1   \n",
       "4               10               0.098765               0.082569     0   \n",
       "\n",
       "                                               title  \\\n",
       "0  IBM Sees Holographic Calls Air Breathing Batte...   \n",
       "1  The Fully Electronic Futuristic Starting Gun T...   \n",
       "2  Fruits that Fight the Flu fruits that fight th...   \n",
       "3                10 Foolproof Tips for Better Sleep    \n",
       "4  The 50 Coolest Jerseys You Didn t Know Existed...   \n",
       "\n",
       "                                                body  \n",
       "0  A sign stands outside the International Busine...  \n",
       "1  And that can be carried on a plane without the...  \n",
       "2  Apples The most popular source of antioxidants...  \n",
       "3  There was a period in my life when I had a lot...  \n",
       "4  Jersey sales is a curious business Whether you...  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = pd.read_csv(\"../../../lesson-13/code/dataset/stumbleupon.tsv\", sep='\\t')\n",
    "data['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\n",
    "data['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bodies = data[\"body\"].dropna('').copy()\n",
    "vectorizer = CountVectorizer(max_features=1000, binary=False, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'00', u'000', u'08', u'09', u'10', u'100', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'2005', u'2006', u'2007', u'2008', u'2009', u'2010', u'2011', u'2012', u'21', u'22', u'23', u'24', u'25', u'26', u'27', u'28', u'29', u'30', u'31', u'35', u'350', u'40', u'45', u'50', u'60', u'ability', u'able', u'according', u'actually', u'ad', u'add', u'added', u'adding', u'addition', u'additional', u'age', u'ago', u'air', u'alcohol', u'allow', u'amazing', u'america', u'american', u'app', u'apple', u'apples', u'april', u'area', u'aren', u'art', u'article', u'aside', u'ask', u'asked', u'athletes', u'attention', u'august', u'author', u'available', u'average', u'avocado', u'avoid', u'away', u'awesome', u'baby', u'background', u'bacon', u'bad', u'bag', u'bake', u'baked', u'baking', u'ball', u'balls', u'banana', u'bar', u'bars', u'base', u'based', u'basic', u'basil', u'batter', u'bean', u'beans', u'beat', u'beautiful', u'beauty', u'beef', u'beer', u'begin', u'believe', u'benefits', u'best', u'better', u'big', u'bit', u'black', u'blend', u'blog', u'blood', u'blue', u'board', u'body', u'boil', u'book', u'bought', u'bowl', u'box', u'brain', u'bread', u'break', u'breakfast', u'breast', u'bring', u'broccoli', u'brown', u'brownies', u'business', u'butter', u'buy', u'buzz', u'cake', u'cakes', u'california', u'called', u'calories', u'came', u'campaign', u'cancer', u'candy', u'car', u'caramel', u'care', u'case', u'category', u'cause', u'cells', u'center', u'certain', u'chance', u'change', u'check', u'cheddar', u'cheese', u'cheesecake', u'chef', u'chicken', u'child', u'children', u'chili', u'chinese', u'chip', u'chips', u'chocolate', u'choice', u'cholesterol', u'choose', u'chopped', u'christmas', u'cinnamon', u'city', u'class', u'classic', u'clean', u'clear', u'click', u'close', u'clothing', u'cloves', u'coat', u'cocoa', u'coconut', u'coffee', u'cold', u'collection', u'college', u'color', u'com', u'combination', u'combine', u'combined', u'come', u'comes', u'coming', u'comment', u'comments', u'common', u'community', u'company', u'complete', u'completely', u'computer', u'concept', u'container', u'content', u'continue', u'control', u'cook', u'cooked', u'cookie', u'cookies', u'cooking', u'cool', u'corn', u'cost', u'couldn', u'country', u'couple', u'course', u'cover', u'covered', u'cream', u'creamy', u'create', u'created', u'crisp', u'crumbs', u'crust', u'culture', u'cup', u'cupcake', u'cupcakes', u'cups', u'cut', u'daily', u'damascus', u'dark', u'data', u'date', u'day', u'days', u'deal', u'death', u'december', u'decided', u'deep', u'definitely', u'degrees', u'delicious', u'design', u'designed', u'dessert', u'desserts', u'did', u'didn', u'diet', u'different', u'dinner', u'dip', u'directions', u'disease', u'dish', u'dishes', u'display', u'div', u'does', u'doesn', u'dog', u'doing', u'don', u'double', u'dough', u'dr', u'dress', u'dried', u'drink', u'drinking', u'drop', u'drug', u'dry', u'early', u'easier', u'easily', u'easy', u'eat', u'eating', u'edges', u'effect', u'effects', u'egg', u'eggs', u'electric', u'email', u'end', u'energy', u'enjoy', u'entire', u'especially', u'evenly', u'event', u'exactly', u'example', u'exercise', u'experience', u'extra', u'extract', u'eye', u'eyes', u'face', u'facebook', u'fact', u'fall', u'false', u'family', u'fan', u'fans', u'far', u'fashion', u'fast', u'fat', u'favorite', u'features', u'february', u'feel', u'feeling', u'feet', u'fiber', u'field', u'filled', u'filling', u'finally', u'fine', u'firm', u'fish', u'fit', u'fitness', u'flashvars', u'flat', u'flavor', u'flavors', u'flour', u'foil', u'fold', u'follow', u'following', u'food', u'foods', u'football', u'form', u'free', u'freezer', u'french', u'fresh', u'fridge', u'fried', u'friend', u'friends', u'frosting', u'frozen', u'fruit', u'fruits', u'fry', u'fun', u'function', u'funny', u'future', u'game', u'games', u'garlic', u'gently', u'gets', u'getting', u'ginger', u'girl', u'given', u'gives', u'glass', u'goes', u'going', u'gold', u'golden', u'good', u'google', u'got', u'grated', u'great', u'green', u'grill', u'grilled', u'ground', u'group', u'guide', u'guy', u'hair', u'half', u'hand', u'hands', u'happy', u'hard', u'having', u'head', u'health', u'healthy', u'heart', u'heat', u'heavy', u'height', u'help', u'helps', u'herbs', u'high', u'history', u'hit', u'hold', u'holiday', u'home', u'homemade', u'honey', u'hope', u'hot', u'hour', u'hours', u'house', u'href', u'http', u'huge', u'human', u'ice', u'idea', u'ideas', u'image', u'images', u'immediately', u'important', u'improve', u'inch', u'inches', u'include', u'including', u'increase', u'industry', u'information', u'ingredients', u'inside', u'inspired', u'instead', u'instructions', u'interesting', u'international', u'internet', u'iphone', u'isn', u'issue', u'italian', u'items', u'january', u'job', u'jpg', u'juice', u'july', u'june', u'just', u'key', u'kids', u'kind', u'kitchen', u'knew', u'knife', u'know', u'known', u'la', u'large', u'later', u'latest', u'layer', u'lead', u'learn', u'leave', u'leaves', u'left', u'lemon', u'let', u'level', u'levels', u'life', u'light', u'lightly', u'like', u'likely', u'lime', u'line', u'link', u'liquid', u'list', u'little', u'live', u'living', u'll', u'local', u'london', u'long', u'longer', u'look', u'looking', u'looks', u'lose', u'loss', u'lost', u'lot', u'lots', u'love', u'low', u'lower', u'lunch', u'magazine', u'main', u'major', u'make', u'makes', u'making', u'man', u'march', u'market', u'matter', u'maybe', u'meal', u'meals', u'mean', u'means', u'meat', u'media', u'medical', u'medicine', u'medium', u'melt', u'melted', u'memory', u'men', u'method', u'microwave', u'middle', u'milk', u'million', u'mind', u'mini', u'minute', u'minutes', u'mix', u'mixed', u'mixer', u'mixing', u'mixture', u'model', u'models', u'mom', u'moment', u'money', u'month', u'months', u'morning', u'mother', u'muffin', u'muscle', u'muscles', u'mushrooms', u'music', u'national', u'natural', u'need', u'needed', u'needs', u'network', u'new', u'news', u'nfl', u'nice', u'night', u'non', u'normal', u'note', u'november', u'number', u'nutella', u'nutrition', u'nuts', u'october', u'offer', u'offers', u'office', u'oh', u'oil', u'old', u'olive', u'olympic', u'olympics', u'ones', u'onion', u'onions', u'online', u'open', u'optional', u'orange', u'order', u'organic', u'original', u'ounces', u'outside', u'oven', u'oz', u'package', u'page', u'pain', u'pan', u'paper', u'parchment', u'parents', u'parmesan', u'party', u'past', u'pasta', u'pastry', u'peanut', u'people', u'pepper', u'peppers', u'percent', u'perfect', u'person', u'personal', u'phone', u'photo', u'photography', u'photos', u'physical', u'pick', u'picture', u'pictures', u'pie', u'piece', u'pieces', u'pink', u'pizza', u'place', u'plan', u'plastic', u'plate', u'play', u'player', u'plus', u'pm', u'point', u'police', u'pop', u'popular', u'pork', u'position', u'possible', u'post', u'posted', u'posts', u'pot', u'potato', u'potatoes', u'pound', u'pounds', u'pour', u'powder', u'powdered', u'power', u'preheat', u'prepare', u'prepared', u'press', u'pressure', u'pretty', u'prevent', u'print', u'probably', u'problem', u'problems', u'process', u'processor', u'produce', u'product', u'products', u'program', u'project', u'protein', u'provide', u'public', u'published', u'pull', u'pumpkin', u'purpose', u'quality', u'question', u'quick', u'quickly', u'quite', u'rack', u'raw', u'read', u'reading', u'ready', u'real', u'really', u'reason', u'recent', u'recently', u'recipe', u'recipes', u'red', u'reduce', u'refrigerator', u'regular', u'related', u'remaining', u'remember', u'remove', u'repeat', u'report', u'research', u'researchers', u'rest', u'restaurant', u'result', u'results', u'return', u'review', u'rice', u'rich', u'right', u'rise', u'risk', u'roasted', u'roll', u'rolls', u'room', u'round', u'run', u'running', u'safe', u'said', u'salad', u'salt', u'san', u'sandwich', u'sauce', u'saucepan', u'save', u'saw', u'say', u'says', u'school', u'science', u'search', u'season', u'second', u'seconds', u'seeds', u'seen', u'self', u'september', u'serve', u'served', u'serving', u'set', u'sex', u'shape', u'share', u'sheet', u'shoes', u'shop', u'short', u'shows', u'shredded', u'shrimp', u'si', u'sides', u'sign', u'similar', u'simmer', u'simple', u'simply', u'single', u'sit', u'site', u'size', u'skillet', u'skin', u'sleep', u'slice', u'sliced', u'slices', u'slightly', u'slow', u'slowly', u'small', u'smooth', u'snack', u'social', u'soda', u'soft', u'soon', u'soup', u'sour', u'source', u'space', u'special', u'speed', u'spend', u'spice', u'spinach', u'spoon', u'sport', u'sports', u'spray', u'spread', u'spring', u'sprinkle', u'stand', u'star', u'start', u'started', u'starting', u'state', u'states', u'static', u'stay', u'steak', u'step', u'stick', u'stir', u'stirring', u'stop', u'store', u'story', u'straight', u'strawberries', u'strawberry', u'street', u'stress', u'strong', u'studies', u'study', u'stuff', u'style', u'sugar', u'summer', u'sun', u'super', u'support', u'sure', u'surface', u'sweet', u'swimsuit', u'syrup', u'table', u'tablespoon', u'tablespoons', u'taken', u'takes', u'taking', u'talk', u'taste', u'tasty', u'tbsp', u'tea', u'team', u'teaspoon', u'teaspoons', u'tech', u'technology', u'tell', u'temperature', u'tender', u'term', u'terminal01', u'test', u'text', u'texture', u'thank', u'thanks', u'thing', u'things', u'think', u'thinking', u'thought', u'throw', u'time', u'times', u'tip', u'tips', u'today', u'told', u'tomato', u'tomatoes', u'took', u'topping', u'toss', u'total', u'touch', u'track', u'traditional', u'training', u'transfer', u'treat', u'treatment', u'tried', u'true', u'try', u'trying', u'tsp', u'turkey', u'turn', u'turned', u'tv', u'twitter', u'type', u'types', u'united', u'university', u'unsalted', u'url', u'use', u'used', u'user', u'username', u'users', u'uses', u'using', u'usually', u'vanilla', u'var', u'variety', u've', u'vegan', u'vegetable', u'vegetables', u'vegetarian', u'version', u'video', u'videos', u'view', u'vinegar', u'vitamin', u'wait', u'want', u'wanted', u'warm', u'wasn', u'watch', u'water', u'way', u'ways', u'wear', u'web', u'website', u'week', u'weeks', u'weight', u'went', u'wheat', u'whipped', u'whisk', u'white', u'whites', u'wide', u'width', u'win', u'wine', u'winter', u'woman', u'women', u'won', u'wonderful', u'words', u'work', u'working', u'workout', u'works', u'world', u'worth', u'wrap', u'wrong', u'www', u'year', u'years', u'yeast', u'yellow', u'yes', u'yogurt', u'york', u'young']\n"
     ]
    }
   ],
   "source": [
    "x = vectorizer.fit_transform(bodies) #combined function\n",
    "#x = vectorizer.transform(bodies)\n",
    "print vectorizer.get_feature_names() #print out words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer_title' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f3be8a75ad87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer_title\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer_title' is not defined"
     ]
    }
   ],
   "source": [
    "id2word = dict(enumerate(vectorizer.get_feature_names()))\n",
    "x_title = vectorizer_title.fit_transfer(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "corpus = Sparse2Corpus(x, documents_columns = False)\n",
    "lda_modal = LdaModel(corpus=corpus, id2word=id2word, num_topics=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "(11, u'0.033*\"food\" + 0.020*\"recipes\" + 0.015*\"recipe\" + 0.012*\"salad\" + 0.011*\"potatoes\"')\n",
      "()\n",
      "Topic: 1\n",
      "(3, u'0.018*\"just\" + 0.017*\"said\" + 0.016*\"like\" + 0.012*\"make\" + 0.011*\"don\"')\n",
      "()\n",
      "Topic: 2\n",
      "(2, u'0.018*\"cup\" + 0.015*\"add\" + 0.014*\"minutes\" + 0.010*\"salt\" + 0.010*\"heat\"')\n",
      "()\n",
      "Topic: 3\n",
      "(7, u'0.031*\"fashion\" + 0.015*\"look\" + 0.015*\"style\" + 0.014*\"dress\" + 0.012*\"new\"')\n",
      "()\n",
      "Topic: 4\n",
      "(5, u'0.019*\"butter\" + 0.018*\"cup\" + 0.015*\"baking\" + 0.014*\"recipe\" + 0.013*\"chocolate\"')\n",
      "()\n",
      "Topic: 5\n",
      "(0, u'0.030*\"video\" + 0.029*\"funny\" + 0.025*\"videos\" + 0.013*\"pictures\" + 0.013*\"best\"')\n",
      "()\n",
      "Topic: 6\n",
      "(13, u'0.042*\"images\" + 0.023*\"background\" + 0.021*\"url\" + 0.018*\"pumpkin\" + 0.018*\"width\"')\n",
      "()\n",
      "Topic: 7\n",
      "(6, u'0.015*\"like\" + 0.014*\"just\" + 0.012*\"time\" + 0.011*\"sports\" + 0.010*\"world\"')\n",
      "()\n",
      "Topic: 8\n",
      "(4, u'0.022*\"2010\" + 0.018*\"2009\" + 0.014*\"new\" + 0.013*\"technology\" + 0.012*\"2008\"')\n",
      "()\n",
      "Topic: 9\n",
      "(8, u'0.033*\"flashvars\" + 0.019*\"fat\" + 0.015*\"weight\" + 0.011*\"food\" + 0.011*\"body\"')\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "num_topics = 10\n",
    "num_words_per_topic = 5\n",
    "for ti, topic in enumerate(lda_modal.show_topics(num_topics = num_topics, num_words = num_words_per_topic)):\n",
    "    print(\"Topic: %d\" %(ti))\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "text = data.body.dropna().map(lambda x: x.split())\n",
    "model = Word2Vec(text, size=100, window=5, min_count=5, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'research,', 0.9041967391967773),\n",
       " (u'relating', 0.9014872908592224),\n",
       " (u'advocacy', 0.9012855291366577),\n",
       " (u'imaging', 0.8934394717216492),\n",
       " (u'holistic', 0.8933953642845154),\n",
       " (u'enterprise', 0.8922281265258789),\n",
       " (u'maximizing', 0.8913795351982117),\n",
       " (u'intellectual', 0.8913322687149048),\n",
       " (u'biz', 0.8910111784934998),\n",
       " (u'prospective', 0.8907525539398193)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['banking'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unicode Handling\n",
    "from __future__ import unicode_literals\n",
    "import codecs\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "# spacy is used for pre-processing and traditional NLP\n",
    "import spacy\n",
    "from spacy.en import English\n",
    "\n",
    "# Gensim is used for LDA and word2vec\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the tweet data\n",
    "filename = '../dataset/captured-tweets.txt'\n",
    "tweets = []\n",
    "for tweet in codecs.open(filename, 'r', encoding=\"utf-8\"):\n",
    "    tweets.append(tweet)\n",
    "# Setting up spacy\n",
    "nlp_toolkit = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet\n",
      "0  I made a(n) Small Tourmaline in Paradise Islan...\n",
      "1  RT @PURELOVEBEAST: -เช็ครายละเอียด- 27th BIRTH...\n",
      "2                          https://t.co/EOfBdVQUfO\\n\n",
      "3                      @ViGiGu google it :) simple\\n\n",
      "4           nerd ass girl  https://t.co/T7kDirxPEL\\n\n"
     ]
    }
   ],
   "source": [
    "tweet_df = pd.DataFrame({'tweet': tweets})\n",
    "print tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1a\n",
    "\n",
    "Write a function that can take a take a sentence parsed by `spacy` and identify if it mentions a company named 'Google'. Remember, `spacy` can find entities and codes them as `ORG` if they are a company. Look at the slides for class 13 if you need a hint:\n",
    "\n",
    "### Bonus (1b)\n",
    "\n",
    "Parameterize the company name so that the function works for any company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jazz/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:18: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tweet\n",
      "8                          Google Play Gift Card Code\\n\n",
      "9     Claim your Google Play Gift Card Code... https...\n",
      "13    King of dark fantasy. Summon today. App Store:...\n",
      "14    King of dark fantasy. Summon today. App Store:...\n",
      "16    I've entered to win a Google Nexus 6P from  ! ...\n",
      "18    RT @kamcb29: I've entered to win a Google Nexu...\n",
      "20    I LOVE your Google plus page with the other gi...\n",
      "21    After I've Google &amp; read a ton of articles...\n",
      "23    RT @ShowerThoughtts: Apple has \"air\", Amazon h...\n",
      "24    -Looks up on Google 'MikexJeremy' secretly- &l...\n",
      "25    RT @_silentbent_: Go support @dadeputy single ...\n",
      "29    Ever wanted to become a Google Small Business ...\n",
      "31    Top Android Apps (without all the games) revea...\n",
      "33    https://t.co/0ElEQm917Q How To Increase Blog T...\n",
      "35                        Europe could produce a Fac...\n",
      "38                        Europe could produce a Fac...\n",
      "40    New post: Google Knowledge Graph Optimization ...\n",
      "41    Anyone else spend the entire time driving tryi...\n",
      "43    Google, you've failed me for the first time! W...\n",
      "48                                    Google #Team2PM\\n\n",
      "53    RT https://t.co/s2HOfLUV2t #Google '#Android N...\n",
      "57    @ImbruedJoint Hector had softly tilted his hea...\n",
      "60    Google Knowledge Graph Optimization Strategies...\n",
      "61    LebanonHashtag: RT probrandz: #Facebook Open S...\n",
      "63    @r0thejan Earn free Google Play and many more ...\n",
      "64    KalGuntuku: RT bunnymiilk \"RT 30seclovelive: [...\n",
      "65    RT @FortuneMagazine: Here’s how Google is taki...\n",
      "67    shenbrood: What's your greatest #digitalmarket...\n",
      "68    https://t.co/tkiiygFXRd What each state #Googl...\n",
      "70    Europe could produce a Facebook - and the Goog...\n",
      "...                                                 ...\n",
      "3182  RT @Allanimeinforma: Top 10 searched keywords ...\n",
      "3186  RT @Scribessy: Tips to Improve Your Business w...\n",
      "3188  Google got its name by accident. The founders ...\n",
      "3190  11 Google tricks that will change the way you ...\n",
      "3191  RT @mindglobal: Like it or not, v wil get more...\n",
      "3193  RT @RetiredFilth: Gotta set a Google calendar ...\n",
      "3195  Come join me in an epic adventure. Invite ID: ...\n",
      "3198  King of dark fantasy. Summon today. App Store:...\n",
      "3199  King of dark fantasy. Summon today. App Store:...\n",
      "3203  RT @mindglobal: Like it or not, v wil get more...\n",
      "3204  King of dark fantasy. Summon today. App Store:...\n",
      "3205  RT @Nickelodeon_AU: Download our app #NickPlay...\n",
      "3206  @twitersgoodboy @IceBergMama Google emoji are ...\n",
      "3207  Google Dorks SQLi Colleciton - https://t.co/1k...\n",
      "3208     RT @GoogleInvestor: Google Up +1.6% for Week\\n\n",
      "3212  New Google Chat App Could Destroy Facebook Mes...\n",
      "3213  Amateur Archaeologist + Google Earth = Bronze ...\n",
      "3214  I added a video to a @YouTube playlist https:/...\n",
      "3216  King of dark fantasy. Summon today. App Store:...\n",
      "3217  RT @ramprasad_c: .@sardesairajdeep It takes 58...\n",
      "3219  King of dark fantasy. Summon today. App Store:...\n",
      "3220  RT @GoogleInvestor: Google Up +$247.35 &amp; +...\n",
      "3222  King of dark fantasy. Summon today. App Store:...\n",
      "3223  King of dark fantasy. Summon today. App Store:...\n",
      "3226  New Event Now On. Get New Rank S Monsters. App...\n",
      "3228  Top Android Apps (without all the games) revea...\n",
      "3230  RT @TechnoBuffalo: Nexus owners report sync is...\n",
      "3979  #Google Taking Aim at Uber in 2016 https://t.c...\n",
      "4807  Microsoft joins Google and Facebook with warni...\n",
      "4898  Predictions 2016: Apple, Tesla, Google, Medium...\n",
      "\n",
      "[1353 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "def mentions_company(tweet):\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    #if(any([word.ent_type_ == 'ORG' for word in parsed])):\n",
    "    if any([str(word) == 'google' for word in parsed]):\n",
    "            return True\n",
    "    return False\n",
    "#     # Return True if the sentence contains an organization and that organization is Google\n",
    "#     for entity in parsed.ents:\n",
    "#         # Fill in code here\n",
    "#     # Otherwise return False\n",
    "#     return False\n",
    "\n",
    "# # 1b\n",
    "\n",
    "def mentions_company(tweet, company='Google'):\n",
    "    # Your code here\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    if any([str(word) == company for word in parsed]):\n",
    "            return True\n",
    "    return False\n",
    "#     # Return True\n",
    "print tweet_df[tweet_df.tweet.map(lambda x: mentions_company(x, \"Google\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1c\n",
    "\n",
    "Write a function that can take a sentence parsed by `spacy` \n",
    "and return the verbs of the sentence (preferably lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_actions(parsed):\n",
    "    actions = []\n",
    "    # Your code here\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1d\n",
    "For each tweet, parse it using spacy and print it out if the tweet has 'release' or 'announce' as a verb. You'll need to use your `mentions_company` and `get_actions` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jazz/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:13: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google &amp; Ford rumored to announce partnership at CES https://t.co/zOgm1NjHhD https://t.co/Gzx81ujqVC\n",
      "\n",
      "\n",
      "Lenovo And Google To Officially Announce Project Tango On January 7 At CES https://t.co/Qvmc34T5QA\n",
      "\n",
      "\n",
      "Lenovo And Google To Officially Announce Project Tango On January 7 At CES https://t.co/GNmL0uw9xl\n",
      "\n",
      "\n",
      "Google's Project Ara Spiral is expected to be released next year January https://t.co/prycPMuGsG\n",
      "\n",
      "\n",
      "#Lenovo And #Google To Officially Announce #ProjectTango On January 7 At CES https://t.co/04seCBKv16\n",
      "\n",
      "\n",
      "RT @GizmoChina: Lenovo And Google To Officially Announce Project Tango On January 7 At CES https://t.co/GNmL0uw9xl\n",
      "\n",
      "\n",
      "Google and Ford to announce partnership on self-driving cars at CES - Fudzilla (blog) https://t.co/6woe56G22Q\n",
      "\n",
      "\n",
      "Google and Ford to announce partnership on self-driving cars at CES - Fudzilla (blog) https://t.co/4hERVJ4zZK\n",
      "\n",
      "\n",
      "Redesigned Google Glass published on FCC website: release date, Price and features - Tampa Bay Review https://t.co/Vdwr4afx3E www.GlassRoo…\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#word.lemma release or announce \n",
    "# def mentions_company(tweet, company='Google'):\n",
    "#     # Your code here\n",
    "#     parsed = nlp_toolkit(tweet)\n",
    "#     if any([str(word) == company for word in parsed]):\n",
    "#             return True\n",
    "#     return False\n",
    "# #     # Return True\n",
    "# print tweet_df[tweet_df.tweet.map(lambda x: mentions_company(x, \"Google\"))]\n",
    "google_tweets = []\n",
    "for tweet in tweets:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    if any([str(word) == \"Google\" for word in parsed]):\n",
    "        if any([((word.lemma_ == 'release') or (word.lemma_ == 'announce')) for word in parsed]) :\n",
    "            print tweet, '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 1e\n",
    "Write a function that identifies countries - HINT: the entity label for countries is GPE (or GeoPolitical Entity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mentions_country(parsed, country):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1f\n",
    "\n",
    "Re-run (d) to find country tweets that discuss 'Iran' announcing or releasing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jazz/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:3: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @nazirumikailu: Iran pressures Nigeria, demands the release of top Shia leader Ibrahim El-Zakzaky https://t.co/z5vVXQyxLN #BBCAfricaLive\n",
      "\n",
      "\n",
      "RT @cerenomri: \"Literally every US ally in Mideast is on brink of hot war w/ Iran, so we're going to release $100 billion to Iran this mont…\n",
      "\n",
      "\n",
      "GOBE! Iran warns Nigeria to release Shiite leader El-Zakzaky - SEE https://t.co/TRshnC6sVU\n",
      "\n",
      "\n",
      "GOBE! Iran warns Nigeria to release Shiite leader El-Zakzaky - SEE https://t.co/SlvcQtk3vE\n",
      "\n",
      "\n",
      "RT @Vatescorp: #Sudan: Ministry of Foreign Affairs announced Khartoum has decided to sever diplomatic ties with #Iran https://t.co/ofQG8gtL…\n",
      "\n",
      "\n",
      "RT @cerenomri: \"Literally every US ally in Mideast is on brink of hot war w/ Iran, so we're going to release $100 billion to Iran this mont…\n",
      "\n",
      "\n",
      "Hhmmm. Iran claiming to have 'warned Nigeria' to release detained Shiite leader.... @afalli\n",
      "\n",
      "\n",
      "RT @KenGardner11: Arab allies are breaking diplomatic relations with Iran. Obama's response? Release $100 billion to Iran. The quintessenti…\n",
      "\n",
      "\n",
      "RT @cerenomri: \"Literally every US ally in Mideast is on brink of hot war w/ Iran, so we're going to release $100 billion to Iran this mont…\n",
      "\n",
      "\n",
      "RT @NEWS957: Saudi Arabia announces it is severing ties with Iran, Canada urging diplomacy https://t.co/7JE6dOHoQV\n",
      "\n",
      "\n",
      "Saudi Arabia Just Announced It Plans On Severing Ties With Iran after a Tumultuous Weekend https://t.co/ERj1bScAn5 via @ijdotcom\n",
      "\n",
      "\n",
      "RT @ahmed: Full transcript of press conference where Saudi FM @AdelAljubeir announced cutting diplomatic relations with Iran https://t.co/L…\n",
      "\n",
      "\n",
      "RT @Ebuka_Mbanefo: We have warned Nigeria to release ZakZaky - Islamic Republic Of Iran - https://t.co/7oBA1G4sYH\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    if any([str(word) == \"Iran\" for word in parsed]):\n",
    "        if any([((word.lemma_ == 'release') or (word.lemma_ == 'announce')) for word in parsed]) :\n",
    "            print tweet, '\\n'\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Build a `word2vec` model of the tweets we have collected using `gensim`.\n",
    "\n",
    "### Exercise 2a:\n",
    "First take the collection of tweets and tokenize them using spacy.\n",
    "\n",
    "* Think about how this should be done. \n",
    "* Should you only use upper-case or lower-case? \n",
    "* Should you remove punctuations or symbols? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4905x1000 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 28464 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text_split = [[x.text if x.pos != spacy.parts_of_speech.VERB else x.lemma_ \n",
    "#               for x in nlp_toolkit(t)] for t in tweets]\n",
    "\n",
    "x = vectorizer.fit_transform(tweets)\n",
    "\n",
    "#x = vectorizer.fit_transform(bodies) #combined function\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2b:\n",
    "Build a `word2vec` model.\n",
    "Test the window size as well - this is how many surrounding words need to be used to model a word. What do you think is appropriate for Twitter? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(text_split, size=100, window=4, min_count=5, workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2c:\n",
    "Test your word2vec model with a few similarity functions. \n",
    "* Find words similar to 'Syria'.\n",
    "* Find words similar to 'war'.\n",
    "* Find words similar to \"Iran\".\n",
    "* Find words similar to 'Verizon'. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'S', 0.993830680847168),\n",
       " (u'https://t.co/6Y96MXdAOD', 0.9908841848373413),\n",
       " (u'Play\\uff1ahttps://t.co', 0.9905699491500854),\n",
       " (u'Rank', 0.9905510544776917),\n",
       " (u'ROYALTY', 0.9896373152732849),\n",
       " (u'Play', 0.9893139600753784),\n",
       " (u'TOP40', 0.9891680479049683),\n",
       " (u'RADIO', 0.9884524941444397),\n",
       " (u'@BrookingsInst', 0.9871058464050293),\n",
       " (u'juice', 0.9864141345024109)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['Google'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2d\n",
    "\n",
    "Adjust the choices / parameters in (b) and (c) as necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Filter tweets to those that mention 'Iran' or similar entities and 'war' or similar entities.\n",
    "* Do this using just spacy.\n",
    "* Do this using word2vec similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using spacy\n",
    "for tweet in tweets:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using word2vec similarity scores\n",
    "for tweet in tweets[:200]:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
